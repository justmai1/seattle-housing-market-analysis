---
title: "\\textbf{Seattle Housing Market Analysis}"
author: "Justin Mai"
output:
  pdf_document:
    toc: true
    latex_engine: xelatex
  html_document: default
geometry: margin=40pt
fontsize: 11pt
header-includes:
  - \usepackage{titling}
  - \usepackage{titlesec}
  - \titlespacing*{\title}{0pt}{0pt}{0pt}
  - \setlength{\droptitle}{-2em}
  - \setlength{\topskip}{0pt} 
mainfont: "Times New Roman"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(leaps)
library(MASS)
```

\newpage

# 1. Abstract

**Keywords**: King County Home Sales, Multiple Linear Regression, Prediction Modeling

\newpage

# 2. Introduction

Within a highly populated like the Great Seattle and King County area, looking to be a first time homeowner or to purchase a home within the area in general is a tall task. Prices can fluctuate based on several factors making it difficult to make financial decisions. The goal of this report is to develop a model that can support home buyers, investors, and real estate agents navigate the housing market by including the variables that are the most impactful to home prices. Using historical home sales data in King County from 2023-2025, this report aims to use machine learning tools used in multiple linear regression to predict prices. Our goals are **(1) Discover the variables and factors that are significant to forecasting King County home sales using linear regression and ANOVA tools (2) See how well our model predicts prices on our test dataset**

# 3. Data

## 3.1 Data Overview

The primary dataset we are using was developed by *Andy Krause* who works as the Director of Valuation and Market Dynamics at Zillow. The dataset he produced was developed with the goal of creating an open access user tool to support in analyzing the housing market. Our dataset consists of 33,333 different observations which represents home sales from 2023 to the start of 2025. Our model will be primarily used to make predictions on `sale_price` and the dataset consists of continuous variables like `land_val`, `sqft`, and `sqft_lot`, and categorical variables like `city`, `zoning`, and `subdivision`. The dataset comes with 45+ predictors for us to use. See https://github.com/andykrause/kingCoData for more details.

```{r include=FALSE, echo=FALSE}
sales <- read.csv("datasets/kingco_sales_23.csv")
```

## 3.2 Data Splitting

To avoid data leakage, the influence of training data on testing data, we are splitting our data before applying any type of manipulation. We will be using a standard 80% by 20% split between training and testing data respectively. The data manipulation and transformations will be applied to the training data before modeling, our optimal model developed by the training data will be used on the testing data.


```{r label="data_splitting", echo=FALSE}
set.seed(101)
n = nrow(sales)

test_indices <- sample(seq_len(n), size = floor(0.2 * n))
```

After applying splitting our data, we have `r nrow(train)` observations in the training data and `r nrow(test)` observations in the testing data which are both considered sufficient amounts of data. We have chosen the split randomly to eliminate bias in splitting and are also using `set.seed(101)` for the experiment to be reproducible. (See appendix for details)

## 3.3 Data Manipulation

Within our data manipulation process, we started by filtering our data to be data from 2023-2025 because that is the scope of the sales data that we want to model off of. We then recognized that there were too many cities for our data to handle and based off our report for correlation, we found that cities within the same region rise in price at around the same rate. This enables us to create a `region` variable that represents multiple cities listed, giving us a more condensed model without forfeiting any values. We then removed variables that we deemed were unnecessary like `longitude` and `latitude`. We also saw from modeling with the full model that the `view` variables were mostly not significant at $\alpha = 0.05$ so we consolidated it into one binary variable that shows the value 1 if the home does have an exotic view or 0 if not. Before finalizing this model, we checked for NA values to see if there were any and if there were any patterns within NA values. We want the data to be an adequate representation of the population, so ensuring that NA values didn't strongly impact one category was important for us. Luckily we didn't identify any NULL values that resulted in bias within the dataset.

```{r label="data_manipulation", echo=FALSE}

sales <- sales %>% 
  filter(sale_date >= "2023-01-01") %>%
  mutate(region = case_when(
    city %in% c("SEATTLE", "SHORELINE", "LAKE FOREST PARK") ~ "Seattle Area",
    city %in% c("BELLEVUE", "REDMOND", "KIRKLAND", "MEDINA", 
                "CLYDE HILL", "YARROW POINT", "HUNTS POINT", 
                "BEAUX ARTS", "NEWCASTLE") ~ "Eastside",
    city %in% c("RENTON", "TUKWILA", "SEA-TAC", "DES MOINES", 
                "BURIEN", "NORMANDY PARK") ~ "South King County",
    city %in% c("AUBURN", "FEDERAL WAY", "ALGONA", "PACIFIC", "KENT") ~ "Southwest King County",
    city %in% c("SAMMAMISH", "ISSAQUAH", "MAPLE VALLEY", "COVINGTON", 
                "BLACK DIAMOND") ~ "Southeast King County",
    city %in% c("WOODINVILLE", "KENMORE", "BOTHELL", "DUVALL") ~ "North King County",
    city %in% c("SNOQUALMIE", "NORTH BEND", "SKYKOMISH", "CARNATION", "ENUMCLAW") ~ "Rural King County",
    city == "MILTON" ~ "Pierce County",
    city == "KING COUNTY" ~ "General King County",
    TRUE ~ "Unknown"
  )) %>%
  mutate(renovated = ifelse(year_reno == 0, 0, 1)) %>%
  mutate(view = ifelse(view_rainier >= 1 | view_olympics >= 1 | view_cascades >= 1 | view_territorial >= 1 | view_skyline >= 1 | view_sound >= 1 | view_lakewash >= 1 | view_lakesamm >= 1, 1, 0)) %>% 
  dplyr::select(-dplyr::any_of(c(
    "view_rainier", "view_olympics", "view_cascades", 
    "view_territorial", "view_skyline", "view_sound", 
    "view_lakewash", "view_lakesamm", "view_other", 
    "view_otherwater", "X.1", "X", "zoning", 
    "subdivision", "year_reno","city"
  ))) %>% 
  dplyr::select(!all_of(c("sale_id", "pinx", "sale_nbr", "sale_warning", 
                    "join_status", "join_year", "latitude", "longitude")))

sales <- sales %>%
  mutate(sale_date = as.Date(sale_date),
         across(where(is.character), as.factor))
```

## 3.3 Data Transformation

**Linear Modeling Assumptions**:

\begin{itemize}
    \item \textbf{Linearity}: The relationship between the predictor and response is linear.
    \item \textbf{Independence}: All observations are independent of one another (pair-wise independence).
    \item \textbf{Homoscedasticity}: The variance of residuals is constant across predictor levels.
    \item \textbf{Residual Normality}: The residuals follow a normal distribution.
\end{itemize}

To start with our tests, we first checked residual normality among our continuous variables when plotting against our response variable of `sale_price`. We can see that the residuals weren't normal with each plot demonstrating left skewness with many high outliers.

```{r echo=FALSE, label="pre-log-transformation"}
train %>%
  pivot_longer(cols = c(sqft, sqft_lot, land_val, imp_val), 
               names_to = "Variable", values_to = "Value") %>% 
ggplot(aes(x = Value, y = sale_price)) +
  geom_point() +
  facet_wrap(~Variable, scales = "free_x") +
  labs(x = "Continuous Variable", y = "Sale Price")
```

To satisfy this test, we applied a log-transformation on our response and continuous variables to normalize the scatterplots.

```{r echo=FALSE, label="post-log-transformation"}
train %>%
  mutate(across(c(sqft, sqft_lot, land_val, imp_val), log, .names = "log_{.col}")) %>%
  pivot_longer(cols = starts_with("log_"), 
               names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value, y = log(sale_price))) +
  geom_point(alpha = 0.5) +
  facet_wrap(~Variable, scales = "free_x") +
  labs(x = "Log Transformed Value", y = "Sale Price")
```

```{r}
sales$log_sale_price = log(sales$sale_price)
sales$log_sqft = log(sales$sqft)
sales$log_sqft_lot = log(sales$sqft_lot)
```


# 4. Model

```{r}
num_vars <- sales %>%
  select_if(is.numeric) %>%
  dplyr::select(-sale_price)

corr_mat <- cor(num_vars, use = "pairwise.complete.obs")

corrplot(corr_mat, method = "circle", tl.cex = 0.7,
         title = "Correlation Matrix of Numeric Predictors",
         mar = c(0,0,1,0))

```

```{r}
sales <- sales %>% 
  dplyr::select(!c(sqft, sqft_lot))
```

## 4.1 Model Assumptions / Diagnostics

## 4.2 Model Selection

```{r}
test <- sales[test_indices,]
train <- sales[-test_indices,]

log_full_model <- lm(log_sale_price ~ .- sale_price, data = sales)
step_log_model <- stepAIC(log_full_model, direction = "both", trace = FALSE)

anova(step_log_model)
```

# 5. Results

```{r}
test_predictions_log <- predict(step_log_model,newdata = test)
test_predictions_sale_price <- exp(test_predictions_log)

results_df <- data.frame(
  actual_sale = test$sale_price,
  predicted_sale = test_predictions_sale_price
)

results_df <- results_df %>% drop_na()

threshold <- quantile(results_df$actual_sale, 0.95)  # Set threshold as 95th percentile
results_df <- results_df %>%
  filter(actual_sale <= threshold)

rmse = sqrt(mean((results_df$actual_sale - results_df$predicted_sale)^2))

ggplot(results_df, aes(x = actual_sale, y = predicted_sale)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Test Set: Predicted vs. Actual Sale's Price",
       x = "Actual Sales Price (USD)",
       y = "Predicted Sales Price (USD)") +
  theme_minimal()


```


# 6. Discussion / Conclusion

\newpage

# 7 Appendix

Data splitting process

```{r eval=FALSE, ref.label="data_splitting"}

```

Data manipulation process

```{r eval=FALSE, ref.label="data_manipulation"}

```

Pre-log transformation

```{r eval=FALSE, ref.label="pre-log-transformation"}

```

Post-log transformation

```{r eval=FALSE, ref.label="post-log-transformation"}

```

